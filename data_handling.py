# create_pathway_features_fixed.py
import pickle
import numpy as np
from collections import Counter, defaultdict
import re
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, davies_bouldin_score
from sklearn.model_selection import train_test_split

def create_fixed_length_features():
    """Cr√©e des features de longueur fixe pour chaque patient"""
    
    print("üß† CR√âATION DE FEATURES DE LONGUEUR FIXE")
    print("="*50)
    
    # Charger les donn√©es
    with open('medical_sequences_pure.pkl', 'rb') as f:
        data = pickle.load(f)
    
    sequences = data['sequences']
    patient_info = data.get('patient_info', {})
    
    print(f"Donn√©es charg√©es: {len(sequences)} patients")
    
    # 1. Identifier les codes les plus discriminants
    print("\nüîç Identification des codes discriminants...")
    
    # Analyser la fr√©quence par pathway
    pathway_code_freq = defaultdict(Counter)
    
    for pid, seq in sequences.items():
        if pid in patient_info and 'Pathway' in patient_info[pid]:
            pathway = patient_info[pid]['Pathway']
            for code in seq[:20]:  # Limiter aux 20 premiers codes
                pathway_code_freq[pathway][code] += 1
    
    # Trouver les codes sp√©cifiques √† chaque pathway
    all_codes = set()
    for pathway in pathway_code_freq:
        all_codes.update(pathway_code_freq[pathway].keys())
    
    print(f"Codes uniques totaux: {len(all_codes)}")
    
    # 2. S√©lectionner les codes les plus discriminants
    n_top_codes = 100  # Nombre de codes √† utiliser comme features
    code_importance = {}
    
    for code in all_codes:
        # Calculer l'entropie de la distribution sur les pathways
        counts = []
        total = 0
        for pathway in range(1, 11):
            count = pathway_code_freq[pathway].get(code, 0)
            counts.append(count)
            total += count
        
        if total > 0:
            # Normaliser
            probs = [c/total for c in counts]
            # √âviter log(0)
            probs = [p if p > 0 else 1e-10 for p in probs]
            # Calculer entropie
            entropy = -sum(p * np.log(p) for p in probs)
            code_importance[code] = entropy
    
    # Les codes les plus discriminants ont une entropie faible
    sorted_codes = sorted(code_importance.items(), key=lambda x: x[1])
    top_codes = [code for code, entropy in sorted_codes[:n_top_codes]]
    
    print(f"Top {len(top_codes)} codes discriminants s√©lectionn√©s")
    
    # 3. Cr√©er des features de longueur fixe
    print("\nüìä Cr√©ation des vecteurs de features...")
    
    # D√©finir les features
    feature_names = []
    feature_vectors = []
    patient_ids = []
    pathways = []
    
    # Features basiques (toujours pr√©sentes)
    base_features = [
        'seq_length',  # Longueur de la s√©quence
        'unique_codes',  # Nombre de codes uniques
        'has_C50',  # Contient ICD:C50
        'has_Z511',  # Contient ICD:Z511
        'has_Z5100',  # Contient ICD:Z5100
        'has_Z5101',  # Contient ICD:Z5101
        'has_ZZLF900',  # Contient CCAM:ZZLF900
        'has_NO_CODE',  # Contient NO_CODE
    ]
    
    # Ajouter les codes discriminants
    for code in top_codes:
        # Simplifier le nom pour la feature
        simple_name = code.replace(':', '_').replace('.', '_')
        base_features.append(f'has_{simple_name}')
    
    # Ajouter des features de position
    for pos in range(5):  # 5 premi√®res positions
        base_features.append(f'pos{pos}_code')
    
    feature_names = base_features.copy()
    
    # 4. Remplir les features pour chaque patient
    print("   Cr√©ation des vecteurs...")
    
    for pid, seq in sequences.items():
        if pid not in patient_info:
            continue
        
        pathway = patient_info[pid].get('Pathway')
        if not pathway:
            continue
        
        # Initialiser le vecteur de features
        features = []
        
        # 1. Features basiques
        features.append(len(seq))  # seq_length
        features.append(len(set(seq)))  # unique_codes
        
        # 2. Pr√©sence de codes importants
        features.append(1 if 'ICD:C50' in seq else 0)  # has_C50
        features.append(1 if 'ICD:Z511' in seq else 0)  # has_Z511
        features.append(1 if 'ICD:Z5100' in seq else 0)  # has_Z5100
        features.append(1 if 'ICD:Z5101' in seq else 0)  # has_Z5101
        features.append(1 if 'CCAM:ZZLF900' in seq else 0)  # has_ZZLF900
        features.append(1 if 'NO_CODE' in seq else 0)  # has_NO_CODE
        
        # 3. Pr√©sence des codes discriminants
        for code in top_codes:
            features.append(1 if code in seq else 0)
        
        # 4. Codes aux premi√®res positions (encod√©s)
        for pos in range(5):
            if pos < len(seq):
                # Encoder le code √† cette position
                code = seq[pos]
                # Utiliser un hash simple
                code_hash = hash(code) % 100
                features.append(code_hash)
            else:
                features.append(-1)  # Valeur pour position vide
        
        # V√©rifier que toutes les features sont des nombres
        features = [float(f) for f in features]
        
        feature_vectors.append(features)
        patient_ids.append(pid)
        pathways.append(pathway)
    
    # Convertir en numpy arrays
    X = np.array(feature_vectors)
    y = np.array(pathways)
    
    print(f"‚úÖ Features cr√©√©es: {X.shape[0]} patients, {X.shape[1]} features")
    
    # 5. Sauvegarder
    feature_data = {
        'X': X,
        'y': y,
        'patient_ids': patient_ids,
        'feature_names': feature_names,
        'top_codes': top_codes
    }
    
    with open('pathway_features_fixed.pkl', 'wb') as f:
        pickle.dump(feature_data, f)
    
    print(f"üìÅ Features sauvegard√©es dans 'pathway_features_fixed.pkl'")
    
    return feature_data

def test_with_randomforest(feature_data):
    """Teste les features avec RandomForest"""
    
    print("\nüß™ TEST AVEC RANDOMFOREST")
    print("="*50)
    
    X = feature_data['X']
    y = feature_data['y']
    
    print(f"Donn√©es: {X.shape[0]} √©chantillons, {X.shape[1]} features")
    print(f"Distribution des pathways: {Counter(y)}")
    
    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}")
    
    # Entra√Æner RandomForest
    clf = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'  # Important pour donn√©es d√©s√©quilibr√©es
    )
    
    print("Entra√Ænement en cours...")
    clf.fit(X_train, y_train)
    
    # Pr√©dictions
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\nüìà R√âSULTATS:")
    print(f"   Accuracy: {accuracy:.3f}")
    
    # Matrice de confusion
    from sklearn.metrics import confusion_matrix, classification_report
    
    print(f"\nüìä Matrice de confusion:")
    cm = confusion_matrix(y_test, y_pred)
    
    # Afficher en format plus lisible
    pathways = sorted(set(y))
    print("   Vrai\\Pr√©dit:", end="")
    for p in pathways:
        print(f"{p:>6}", end="")
    print()
    
    for i, true_p in enumerate(pathways):
        print(f"   {true_p:>11}", end="")
        for j, pred_p in enumerate(pathways):
            print(f"{cm[i, j]:>6}", end="")
        print()
    
    # Rapport de classification
    print(f"\nüìã Rapport de classification:")
    report = classification_report(y_test, y_pred, digits=3)
    print(report)
    
    # Importance des features
    print(f"\nüîç TOP 20 FEATURES IMPORTANTES:")
    importances = clf.feature_importances_
    indices = np.argsort(importances)[::-1][:20]
    
    for idx in indices:
        if idx < len(feature_data['feature_names']):
            feat_name = feature_data['feature_names'][idx]
            print(f"   {feat_name}: {importances[idx]:.4f}")
    
    return accuracy, clf

def create_embeddings_from_features(feature_data, clf):
    """Cr√©e des embeddings √† partir des features et du mod√®le"""
    
    print("\nüß¨ CR√âATION D'EMBEDDINGS")
    print("="*50)
    
    X = feature_data['X']
    patient_ids = feature_data['patient_ids']
    
    # Utiliser les pr√©dictions probabilistes comme embeddings
    print("Calcul des probabilit√©s...")
    probabilities = clf.predict_proba(X)
    
    # Cr√©er le dict d'embeddings
    embeddings_dict = {}
    for i, pid in enumerate(patient_ids):
        embeddings_dict[pid] = probabilities[i]
    
    print(f"Embeddings cr√©√©s: {len(embeddings_dict)} patients")
    print(f"Dimension embeddings: {probabilities.shape[1]}")
    
    # Sauvegarder
    embeddings_data = {
        'embeddings': embeddings_dict,
        'patient_ids': patient_ids,
        'pathways': feature_data['y'].tolist(),
        'feature_importances': clf.feature_importances_.tolist()
    }
    
    with open('pathway_embeddings_from_rf.pkl', 'wb') as f:
        pickle.dump(embeddings_data, f)
    
    print("üìÅ Embeddings sauvegard√©s dans 'pathway_embeddings_from_rf.pkl'")
    
    return embeddings_dict

def cluster_pathway_embeddings():
    """Clustering sur les nouveaux embeddings avec Davies-Bouldin"""
    
    print("\nüéØ CLUSTERING SUR NOUVEAUX EMBEDDINGS")
    print("="*50)
    
    # Charger les embeddings
    with open('pathway_embeddings_from_rf.pkl', 'rb') as f:
        embeddings_data = pickle.load(f)
    
    embeddings_dict = embeddings_data['embeddings']
    patient_ids = embeddings_data['patient_ids']
    pathways = embeddings_data['pathways']
    
    # Convertir en matrice
    emb_matrix = np.array([embeddings_dict[pid] for pid in patient_ids])
    
    print(f"Embeddings shape: {emb_matrix.shape}")
    
    # Clustering avec diff√©rents K pour trouver l'optimal
    from sklearn.cluster import KMeans
    
    print("\nüîç Recherche du K optimal...")
    
    # Tester diff√©rents K
    k_range = range(2, min(16, emb_matrix.shape[0] // 10 + 1))
    silhouette_scores = []
    davies_bouldin_scores = []
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=25, max_iter=300)
        cluster_labels = kmeans.fit_predict(emb_matrix)
        
        # Calculer les scores
        from sklearn.metrics import silhouette_score
        sil_score = silhouette_score(emb_matrix, cluster_labels)
        db_score = davies_bouldin_score(emb_matrix, cluster_labels)
        
        silhouette_scores.append(sil_score)
        davies_bouldin_scores.append(db_score)
        
        print(f"   K={k}: Silhouette={sil_score:.3f}, Davies-Bouldin={db_score:.3f}")
    
    # Meilleur K selon Silhouette (plus haut = mieux)
    best_k_sil = k_range[np.argmax(silhouette_scores)]
    # Meilleur K selon Davies-Bouldin (plus bas = mieux)
    best_k_db = k_range[np.argmin(davies_bouldin_scores)]
    
    print(f"\nüìä Meilleur K (Silhouette): {best_k_sil}")
    print(f"üìä Meilleur K (Davies-Bouldin): {best_k_db}")
    
    # Choisir le K final (compromis)
    if best_k_sil == best_k_db:
        optimal_k = best_k_sil
        print(f"\n‚úÖ Consensus: K optimal = {optimal_k}")
    else:
        # Prendre la moyenne ou le K recommand√© par Silhouette
        optimal_k = best_k_sil
        print(f"\n‚ö†Ô∏è  D√©saccord entre m√©triques, utilisation de K={optimal_k} (bas√© sur Silhouette)")
    
    # Clustering final avec K optimal
    print(f"\nüî® Clustering final avec K={optimal_k}...")
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=25)
    cluster_labels = kmeans.fit_predict(emb_matrix)
    
    # √âvaluation finale
    from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
    
    ari = adjusted_rand_score(pathways, cluster_labels)
    nmi = normalized_mutual_info_score(pathways, cluster_labels)
    sil_final = silhouette_score(emb_matrix, cluster_labels)
    db_final = davies_bouldin_score(emb_matrix, cluster_labels)
    
    print(f"\nüìà R√âSULTATS FINAUX CLUSTERING:")
    print(f"   ARI: {ari:.3f}")
    print(f"   NMI: {nmi:.3f}")
    print(f"   Silhouette Score: {sil_final:.3f}")
    print(f"   Davies-Bouldin Index: {db_final:.3f}")
    
    # Analyser la correspondance clusters ‚Üî pathways
    from collections import Counter
    
    print(f"\nüîç CORRESPONDANCE CLUSTERS ‚Üî PATHWAYS:")
    
    for cluster_id in range(optimal_k):
        # Indices des patients dans ce cluster
        cluster_indices = np.where(cluster_labels == cluster_id)[0]
        
        if len(cluster_indices) > 0:
            # Pathways dans ce cluster
            cluster_pathways = [pathways[i] for i in cluster_indices]
            pathway_counts = Counter(cluster_pathways)
            
            print(f"\n   Cluster {cluster_id} ({len(cluster_indices)} patients):")
            for pathway, count in pathway_counts.most_common(3):
                proportion = count / len(cluster_indices)
                print(f"      Pathway {pathway}: {count} ({proportion:.1%})")
    
    # Sauvegarder les r√©sultats
    results = {
        'k_range': list(k_range),
        'silhouette_scores': silhouette_scores,
        'davies_bouldin_scores': davies_bouldin_scores,
        'optimal_k': optimal_k,
        'cluster_labels': cluster_labels.tolist(),
        'ari': ari,
        'nmi': nmi,
        'silhouette': sil_final,
        'davies_bouldin': db_final
    }
    
    with open('rf_clustering_results.pkl', 'wb') as f:
        pickle.dump(results, f)
    
    print(f"\nüìÅ R√©sultats sauvegard√©s dans 'rf_clustering_results.pkl'")
    
    return cluster_labels

# Pipeline complet
def main():
    print("üöÄ PIPELINE COMPLET POUR AM√âLIORER LE CLUSTERING")
    print("="*60)
    
    # 1. Cr√©er des features de longueur fixe
    print("\n1. üìä Cr√©ation des features...")
    feature_data = create_fixed_length_features()
    
    # 2. Tester avec RandomForest
    print("\n2. üß™ Test avec RandomForest...")
    accuracy, clf = test_with_randomforest(feature_data)
    
    # 3. Cr√©er des embeddings √† partir du mod√®le
    print("\n3. üß¨ Cr√©ation des embeddings...")
    embeddings_dict = create_embeddings_from_features(feature_data, clf)
    
    # 4. Clustering avec Davies-Bouldin
    print("\n4. üéØ Clustering avec analyse Davies-Bouldin...")
    cluster_labels = cluster_pathway_embeddings()
    
    print(f"\n" + "="*60)
    print(f"‚úÖ PIPELINE TERMIN√â!")
    
    if accuracy > 0.7:
        print("üéâ Bonne discrimination obtenue!")
        print("‚û°Ô∏è  Les nouveaux embeddings devraient donner de meilleurs clusters")
    else:
        print("‚ö†Ô∏è  Discrimination moyenne")
        print("‚û°Ô∏è  Consid√©rez ajouter plus de features ou utiliser un autre mod√®le")

# Version simple pour test rapide
def quick_test():
    """Test rapide sans toutes les √©tapes"""
    
    print("‚ö° TEST RAPIDE")
    
    # Cr√©er features simples
    with open('medical_sequences_pure.pkl', 'rb') as f:
        data = pickle.load(f)
    
    sequences = data['sequences']
    patient_info = data.get('patient_info', {})
    
    # Features tr√®s simples
    X = []
    y = []
    patient_ids = []
    
    for pid, seq in sequences.items():
        if pid in patient_info and 'Pathway' in patient_info[pid]:
            pathway = patient_info[pid]['Pathway']
            
            # 5 features simples
            features = [
                len(seq),  # Longueur
                1 if 'ICD:C50' in seq else 0,  # Cancer sein
                1 if 'ICD:Z511' in seq else 0,  # Chimioth√©rapie
                1 if 'CCAM:ZZLF900' in seq else 0,  # Acte technique
                1 if 'NO_CODE' in seq else 0,  # Trous
            ]
            
            X.append(features)
            y.append(pathway)
            patient_ids.append(pid)
    
    X = np.array(X)
    y = np.array(y)
    
    print(f"Features cr√©√©es: {X.shape}")
    
    # RandomForest rapide
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    clf = RandomForestClassifier(n_estimators=50, random_state=42)
    clf.fit(X_train, y_train)
    
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\nAccuracy avec 5 features simples: {accuracy:.3f}")
    
    return accuracy

if __name__ == "__main__":
    print("Options:")
    print("1. Pipeline complet (avec Davies-Bouldin)")
    print("2. Test rapide avec 5 features")
    print("3. Juste cr√©er les features")
    
    choice = input("\nVotre choix (1-3): ").strip()
    
    if choice == "1":
        main()
    elif choice == "2":
        accuracy = quick_test()
        if accuracy > 0.7:
            print("\n‚úÖ Bon d√©part! Essayez le pipeline complet.")
        else:
            print("\n‚ö†Ô∏è  Features trop simples. Essayez avec plus de features.")
    elif choice == "3":
        feature_data = create_fixed_length_features()
        print(f"\n‚úÖ Features cr√©√©es: {feature_data['X'].shape}")
    else:
        print("‚ùå Choix invalide")